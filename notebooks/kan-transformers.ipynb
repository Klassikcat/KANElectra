{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7419eafc-5a18-4929-9d78-85dff0e86b4a",
   "metadata": {},
   "source": [
    "# Install Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48591c90-7fab-482c-83b6-fce90a7d2afa",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b5696f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, ElectraForMaskedLM, AutoTokenizer\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained('google/electra-base-generator')\n",
    "generator_config = AutoConfig.from_pretrained('google/electra-base-generator')\n",
    "\n",
    "\n",
    "discriminator_tokenizer = AutoTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "discriminator_config = AutoConfig.from_pretrained('google/electra-base-discriminator')\n",
    "\n",
    "\n",
    "random_input_ids = torch.randint(0, len(generator_tokenizer), (1, 512))\n",
    "random_attention_mask = torch.randint(0, 1, (1, 512))\n",
    "random_token_type_ids = torch.randint(0, 1, (1, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dcb981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# reference: efficient-kan by @Blealtan\n",
    "# CODE: https://github.com/Blealtan/efficient-kan\n",
    "\n",
    "\n",
    "class KANLinear(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        enable_standalone_scale_spline=True,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KANLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
    "        grid = (\n",
    "            (\n",
    "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
    "                + grid_range[0]\n",
    "            )\n",
    "            .expand(in_features, -1)\n",
    "            .contiguous()\n",
    "        )\n",
    "        self.register_buffer(\"grid\", grid)\n",
    "\n",
    "        self.base_weight = torch.nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features)\n",
    "        )\n",
    "        self.spline_weight = torch.nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
    "        )\n",
    "        if enable_standalone_scale_spline:\n",
    "            self.spline_scaler = torch.nn.Parameter(\n",
    "                torch.Tensor(out_features, in_features)\n",
    "            )\n",
    "\n",
    "        self.scale_noise = scale_noise\n",
    "        self.scale_base = scale_base\n",
    "        self.scale_spline = scale_spline\n",
    "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
    "        self.base_activation = base_activation()\n",
    "        self.grid_eps = grid_eps\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.base_weight, gain=self.scale_base)\n",
    "        with torch.no_grad():\n",
    "            noise = (\n",
    "                (\n",
    "                    torch.rand(\n",
    "                        self.grid_size + 1, self.in_features, self.out_features\n",
    "                    )\n",
    "                    - 1 / 2\n",
    "                )\n",
    "                * self.scale_noise\n",
    "                / self.grid_size\n",
    "            )\n",
    "            self.spline_weight.data.copy_(\n",
    "                (\n",
    "                    self.scale_spline\n",
    "                    if not self.enable_standalone_scale_spline\n",
    "                    else 1.0\n",
    "                )\n",
    "                * self.curve2coeff(\n",
    "                    self.grid.T[self.spline_order : -self.spline_order],\n",
    "                    noise,\n",
    "                )\n",
    "            )\n",
    "            if self.enable_standalone_scale_spline:\n",
    "                torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n",
    "\n",
    "    def b_splines(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the B-spline bases for the given input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: B-spline bases tensor of shape \\\n",
    "                (batch_size, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        grid: torch.Tensor = (\n",
    "            self.grid  # type: ignore\n",
    "        )  # (in_features, grid_size + 2 * spline_order + 1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
    "        for k in range(1, self.spline_order + 1):\n",
    "            bases = (\n",
    "                (x - grid[:, : -(k + 1)])\n",
    "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
    "                * bases[:, :, :-1]\n",
    "            ) + (\n",
    "                (grid[:, k + 1 :] - x)\n",
    "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
    "                * bases[:, :, 1:]\n",
    "            )\n",
    "\n",
    "        assert bases.size() == (\n",
    "            x.size(0),\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return bases.contiguous()\n",
    "\n",
    "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the coefficients of the curve that interpolates the given\n",
    "        points.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "            y (torch.Tensor): Output tensor of shape \\\n",
    "                (batch_size, in_features, out_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Coefficients tensor of shape \\\n",
    "                (out_features, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
    "\n",
    "        A = self.b_splines(x).transpose(\n",
    "            0, 1\n",
    "        )  # (in_features, batch_size, grid_size + spline_order)\n",
    "        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n",
    "        solution = torch.linalg.lstsq(\n",
    "            A, B\n",
    "        ).solution  # (in_features, grid_size + spline_order, out_features)\n",
    "        result = solution.permute(\n",
    "            2, 0, 1\n",
    "        )  # (out_features, in_features, grid_size + spline_order)\n",
    "\n",
    "        assert result.size() == (\n",
    "            self.out_features,\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return result.contiguous()\n",
    "\n",
    "    @property\n",
    "    def scaled_spline_weight(self):\n",
    "        return self.spline_weight * (\n",
    "            self.spline_scaler.unsqueeze(-1)\n",
    "            if self.enable_standalone_scale_spline\n",
    "            else 1.0\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
    "        spline_output = F.linear(\n",
    "            self.b_splines(x).view(x.size(0), -1),\n",
    "            self.scaled_spline_weight.view(self.out_features, -1),\n",
    "        )\n",
    "        return base_output + spline_output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        batch = x.size(0)\n",
    "\n",
    "        splines = self.b_splines(x)  # (batch, in, coeff)\n",
    "        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n",
    "        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n",
    "        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n",
    "        unreduced_spline_output = torch.bmm(\n",
    "            splines, orig_coeff\n",
    "        )  # (in, batch, out)\n",
    "        unreduced_spline_output = unreduced_spline_output.permute(\n",
    "            1, 0, 2\n",
    "        )  # (batch, in, out)\n",
    "\n",
    "        # sort each channel individually to collect data distribution\n",
    "        x_sorted = torch.sort(x, dim=0)[0]\n",
    "        grid_adaptive = x_sorted[\n",
    "            torch.linspace(\n",
    "                0,\n",
    "                batch - 1,\n",
    "                self.grid_size + 1,\n",
    "                dtype=torch.int64,\n",
    "                device=x.device,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        uniform_step = (\n",
    "            x_sorted[-1] - x_sorted[0] + 2 * margin\n",
    "        ) / self.grid_size\n",
    "        grid_uniform = (\n",
    "            torch.arange(\n",
    "                self.grid_size + 1, dtype=torch.float32, device=x.device\n",
    "            ).unsqueeze(1)\n",
    "            * uniform_step\n",
    "            + x_sorted[0]\n",
    "            - margin\n",
    "        )\n",
    "\n",
    "        grid = (\n",
    "            self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
    "        )\n",
    "        grid = torch.concatenate(\n",
    "            [\n",
    "                grid[:1]\n",
    "                - uniform_step\n",
    "                * torch.arange(\n",
    "                    self.spline_order, 0, -1, device=x.device\n",
    "                ).unsqueeze(1),\n",
    "                grid,\n",
    "                grid[-1:]\n",
    "                + uniform_step\n",
    "                * torch.arange(\n",
    "                    1, self.spline_order + 1, device=x.device\n",
    "                ).unsqueeze(1),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        self.grid.copy_(grid.T)  # type: ignore\n",
    "        self.spline_weight.data.copy_(\n",
    "            self.curve2coeff(x, unreduced_spline_output)\n",
    "        )\n",
    "\n",
    "    def regularization_loss(\n",
    "        self, regularize_activation=1.0, regularize_entropy=1.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute the regularization loss.\n",
    "\n",
    "        This is a dumb simulation of the original L1 regularization as\n",
    "        stated in the paper, since the original one requires computing\n",
    "        absolutes and entropy from the expanded\n",
    "        (batch, in_features, out_features) intermediate tensor, which is\n",
    "        hidden behind the F.linear function if we want an memory\n",
    "        efficient implementation.\n",
    "\n",
    "        The L1 regularization is now computed as mean absolute value of the\n",
    "        spline weights. The authors implementation also includes this term\n",
    "        in addition to the sample-based regularization.\n",
    "        \"\"\"\n",
    "        l1_fake = self.spline_weight.abs().mean(-1)\n",
    "        regularization_loss_activation = l1_fake.sum()\n",
    "        p = l1_fake / regularization_loss_activation\n",
    "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
    "        return (\n",
    "            regularize_activation * regularization_loss_activation\n",
    "            + regularize_entropy * regularization_loss_entropy\n",
    "        )\n",
    "\n",
    "\n",
    "class KAN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        width,\n",
    "        grid=3,\n",
    "        k=3,\n",
    "        noise_scale=0.1,\n",
    "        noise_scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        base_fun=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "        bias_trainable=True,\n",
    "    ):\n",
    "        super(KAN, self).__init__()\n",
    "        self.grid_size = grid\n",
    "        self.spline_order = k\n",
    "        self.bias_trainable = bias_trainable  # TODO\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for in_features, out_features in zip(width, width[1:]):\n",
    "            self.layers.append(\n",
    "                KANLinear(\n",
    "                    in_features,\n",
    "                    out_features,\n",
    "                    grid_size=grid,\n",
    "                    spline_order=grid,\n",
    "                    scale_noise=noise_scale,\n",
    "                    scale_base=noise_scale_base,\n",
    "                    scale_spline=scale_spline,\n",
    "                    base_activation=base_fun,\n",
    "                    grid_eps=grid_eps,\n",
    "                    grid_range=grid_range,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, update_grid=False):\n",
    "        B, C, T = x.shape\n",
    "\n",
    "        x = x.view(-1, T)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if update_grid:\n",
    "                layer.update_grid(x)\n",
    "            x = layer(x)\n",
    "\n",
    "        U = x.shape[1]\n",
    "\n",
    "        x = x.view(B, C, U)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def regularization_loss(\n",
    "        self, regularize_activation=1.0, regularize_entropy=1.0\n",
    "    ):\n",
    "        return sum(\n",
    "            layer.regularization_loss(\n",
    "                regularize_activation, regularize_entropy\n",
    "            )\n",
    "            for layer in self.layers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2f8155f-343c-43f2-b451-4572ac1f8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import *\n",
    "import torch\n",
    "from torch import (\n",
    "    nn, \n",
    "    Tensor, \n",
    "    FloatTensor, \n",
    "    LongTensor\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ElectraGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        vocab_type_size: int,\n",
    "        embedding_dropout_p: float,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        ff_dim: int,\n",
    "        num_layers: int,\n",
    "        max_pos_embedding: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = InputEmbedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            vocab_type_size,\n",
    "            embedding_dropout_p,\n",
    "            max_pos_embedding\n",
    "        )\n",
    "        self.encoder = ElectraEncoder(\n",
    "            hidden_dim,\n",
    "            num_heads,\n",
    "            num_layers,\n",
    "            0.1,\n",
    "            ff_dim\n",
    "        )\n",
    "        self.generator = GeneratorOutput(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: LongTensor, \n",
    "        attention_mask: LongTensor,\n",
    "        token_type_ids: LongTensor,\n",
    "    ) -> Tensor:\n",
    "        embeddings = self.embedding(input_ids, token_type_ids)\n",
    "        seq_out = self.encoder(embeddings, attention_mask)\n",
    "        dropouted_seq_output = F.dropout(seq_out, p=0.1)\n",
    "        return self.generator(dropouted_seq_output)\n",
    "    \n",
    "\n",
    "class GeneratorOutput(nn.Module):\n",
    "    def __init__(self, hidden, vocab_size) :\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        return self.softmax(self.linear(x))\n",
    "    \n",
    "    \n",
    "class ElectraDiscriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        vocab_type_size: int,\n",
    "        embedding_dropout_p: float,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        ff_dim: int,\n",
    "        num_layers: int,\n",
    "        max_pos_embedding: int,\n",
    "        num_labels: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = InputEmbedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            vocab_type_size,\n",
    "            embedding_dropout_p,\n",
    "            max_pos_embedding\n",
    "        )\n",
    "        self.encoder = ElectraEncoder(\n",
    "            hidden_dim,\n",
    "            num_heads,\n",
    "            num_layers,\n",
    "            0.1,\n",
    "            ff_dim\n",
    "        )\n",
    "        self.classifier = KAN(width=[hidden_dim, num_labels])\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: LongTensor, \n",
    "        attention_mask: LongTensor,\n",
    "        token_type_ids: LongTensor,\n",
    "    ) -> Tensor:\n",
    "        embeddings = self.embedding(input_ids, token_type_ids)\n",
    "        seq_out = self.encoder(embeddings, attention_mask)\n",
    "        dropouted_seq_output = F.dropout(seq_out, p=0.1)\n",
    "        return self.classifier(dropouted_seq_output)\n",
    "    \n",
    "\n",
    "class ElectraEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        dropout_p: float = 0.1,\n",
    "        hidden_dim: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not hidden_dim:\n",
    "            hidden_dim = dim * 4 # default hidden_dim on paper\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(dim, num_heads, hidden_dim, dropout_p) for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Tensor,\n",
    "        mask: Tensor\n",
    "    ) -> Tensor:\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, mask)\n",
    "        return hidden_states\n",
    "\n",
    "    \n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        vocab_type_size: int,\n",
    "        embedding_dropout_p: float,\n",
    "        max_pos_embedding: int\n",
    "        ):\n",
    "       super().__init__()\n",
    "       self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "       self.positional_embedding = nn.Embedding(max_pos_embedding, embedding_dim)\n",
    "       self.token_type_embedding = nn.Embedding(vocab_type_size, embedding_dim)\n",
    "       self.dropout = nn.Dropout(embedding_dropout_p)\n",
    "   \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: LongTensor, \n",
    "        token_type_ids: LongTensor,\n",
    "    ) -> Tensor:\n",
    "        seq_length = input_ids.shape[1]\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        \n",
    "        embeddings = (\n",
    "            self.embedding(input_ids) +\n",
    "            self.positional_embedding(position_ids) +\n",
    "            self.token_type_embedding(token_type_ids)\n",
    "        )\n",
    "        return self.dropout(embeddings)\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        query: Tensor, \n",
    "        key: Tensor,\n",
    "        value: Tensor,\n",
    "        attention_mask: LongTensor\n",
    "    ) -> Tensor:\n",
    "        multiplied_kv = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(key.shape[-1])\n",
    "        masked_attention = multiplied_kv.masked_fill(attention_mask == 0, -1e9)\n",
    "        attention = self.softmax(masked_attention)\n",
    "        return torch.matmul(attention, value)\n",
    "        \n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        dropout_p: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.attention = ScaledDotProductAttention(dropout_p)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc_q = KAN(width=[dim, dim])\n",
    "        self.fc_k = KAN(width=[dim, dim])\n",
    "        self.fc_v = KAN(width=[dim, dim])\n",
    "        self.fc_out = KAN(width=[dim, dim])\n",
    "        self.num_heads = num_heads \n",
    "        self.dim = dim\n",
    "               \n",
    "    def forward(\n",
    "        self, \n",
    "        query: Tensor, \n",
    "        key: Tensor,\n",
    "        value: Tensor,\n",
    "        attention_mask: LongTensor\n",
    "    ) -> Tensor:\n",
    "        batch_size = query.size(0)\n",
    "        query = self.fc_q(query).view(batch_size, -1, self.num_heads, query.size(-1) // self.num_heads).transpose(1, 2)\n",
    "        key = self.fc_k(key).view(batch_size, -1, self.num_heads, key.size(-1) // self.num_heads).transpose(1, 2)\n",
    "        value = self.fc_v(value).view(batch_size, -1, self.num_heads, value.size(-1) // self.num_heads).transpose(1, 2)\n",
    "        attention_output = self.attention(query, key, value, attention_mask)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * (self.dim // self.num_heads))\n",
    "        output = self.fc_out(attention_output)\n",
    "        return self.dropout(output)\n",
    " \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        ff_dim: int,\n",
    "        dropout_p: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, ff_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ff_dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor\n",
    "    ) -> Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        dropout_p: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(dim, num_heads, dropout_p)\n",
    "        self.ff = FeedForward(dim, hidden_dim, dropout_p)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor, \n",
    "        attention_mask: LongTensor\n",
    "    ) -> Tensor:\n",
    "        attention_output = self.attn(x, x, x, attention_mask)\n",
    "        add_norm = self.norm1(x + attention_output)\n",
    "        output = self.ff(attention_output)\n",
    "        ff_add_norm = self.norm2(add_norm + output)\n",
    "        return self.dropout(ff_add_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6703ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ElectraGenerator(\n",
    "    vocab_size=len(generator_tokenizer.vocab),\n",
    "    embedding_dim=768,\n",
    "    vocab_type_size=2,\n",
    "    embedding_dropout_p=0.1,\n",
    "    hidden_dim=768,\n",
    "    num_heads=12,\n",
    "    ff_dim=3072,\n",
    "    num_layers=12,\n",
    "    max_pos_embedding=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "abf21a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-10.2888, -10.1096, -10.2493,  ..., -10.4782, -10.6625, -11.6201],\n",
       "         [-10.6305, -10.6472,  -9.5383,  ..., -10.5495, -10.6787, -11.3437],\n",
       "         [ -9.6382, -10.6633, -11.2493,  ..., -10.0595, -10.3957, -11.0472],\n",
       "         ...,\n",
       "         [ -9.9740, -10.3092, -11.2069,  ..., -10.3012, -10.2389, -11.5876],\n",
       "         [ -9.9291, -10.1871, -10.6377,  ...,  -9.8839, -10.5111, -11.5354],\n",
       "         [ -9.6093,  -9.6126,  -9.9465,  ...,  -9.4402, -10.1106, -11.5572]]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(random_input_ids, random_attention_mask, random_token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "208bb447-505d-469f-b00a-072d569da746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(orig_output.logits, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
