{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7419eafc-5a18-4929-9d78-85dff0e86b4a",
   "metadata": {},
   "source": [
    "# Install Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48591c90-7fab-482c-83b6-fce90a7d2afa",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b5696f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/kan-electra/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, ElectraForMaskedLM, AutoTokenizer\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained('google/electra-base-generator')\n",
    "generator_config = AutoConfig.from_pretrained('google/electra-base-generator')\n",
    "\n",
    "\n",
    "discriminator_tokenizer = AutoTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "discriminator_config = AutoConfig.from_pretrained('google/electra-base-discriminator')\n",
    "text = \"Hello, this is [MASK] speaking, how can I help you?\"\n",
    "\n",
    "inputs = generator_tokenizer(text, return_tensors='pt', padding='max_length', max_length=512, truncation=True)\n",
    "random_input_ids = inputs['input_ids']\n",
    "random_attention_mask = inputs['attention_mask']\n",
    "random_token_type_ids = inputs['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0dcb981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# reference: efficient-kan by @Blealtan\n",
    "# CODE: https://github.com/Blealtan/efficient-kan\n",
    "\n",
    "\n",
    "class KANLinear(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        enable_standalone_scale_spline=True,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KANLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
    "        grid = (\n",
    "            (\n",
    "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
    "                + grid_range[0]\n",
    "            )\n",
    "            .expand(in_features, -1)\n",
    "            .contiguous()\n",
    "        )\n",
    "        self.register_buffer(\"grid\", grid)\n",
    "\n",
    "        self.base_weight = torch.nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features)\n",
    "        )\n",
    "        self.spline_weight = torch.nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
    "        )\n",
    "        if enable_standalone_scale_spline:\n",
    "            self.spline_scaler = torch.nn.Parameter(\n",
    "                torch.Tensor(out_features, in_features)\n",
    "            )\n",
    "\n",
    "        self.scale_noise = scale_noise\n",
    "        self.scale_base = scale_base\n",
    "        self.scale_spline = scale_spline\n",
    "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
    "        self.base_activation = base_activation()\n",
    "        self.grid_eps = grid_eps\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.base_weight, gain=self.scale_base)\n",
    "        with torch.no_grad():\n",
    "            noise = (\n",
    "                (\n",
    "                    torch.rand(\n",
    "                        self.grid_size + 1, self.in_features, self.out_features\n",
    "                    )\n",
    "                    - 1 / 2\n",
    "                )\n",
    "                * self.scale_noise\n",
    "                / self.grid_size\n",
    "            )\n",
    "            self.spline_weight.data.copy_(\n",
    "                (\n",
    "                    self.scale_spline\n",
    "                    if not self.enable_standalone_scale_spline\n",
    "                    else 1.0\n",
    "                )\n",
    "                * self.curve2coeff(\n",
    "                    self.grid.T[self.spline_order : -self.spline_order],\n",
    "                    noise,\n",
    "                )\n",
    "            )\n",
    "            if self.enable_standalone_scale_spline:\n",
    "                torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n",
    "\n",
    "    def b_splines(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the B-spline bases for the given input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: B-spline bases tensor of shape \\\n",
    "                (batch_size, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        grid: torch.Tensor = (\n",
    "            self.grid  # type: ignore\n",
    "        )  # (in_features, grid_size + 2 * spline_order + 1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
    "        for k in range(1, self.spline_order + 1):\n",
    "            bases = (\n",
    "                (x - grid[:, : -(k + 1)])\n",
    "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
    "                * bases[:, :, :-1]\n",
    "            ) + (\n",
    "                (grid[:, k + 1 :] - x)\n",
    "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
    "                * bases[:, :, 1:]\n",
    "            )\n",
    "\n",
    "        assert bases.size() == (\n",
    "            x.size(0),\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return bases.contiguous()\n",
    "\n",
    "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the coefficients of the curve that interpolates the given\n",
    "        points.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "            y (torch.Tensor): Output tensor of shape \\\n",
    "                (batch_size, in_features, out_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Coefficients tensor of shape \\\n",
    "                (out_features, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
    "\n",
    "        A = self.b_splines(x).transpose(\n",
    "            0, 1\n",
    "        )  # (in_features, batch_size, grid_size + spline_order)\n",
    "        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n",
    "        solution = torch.linalg.lstsq(\n",
    "            A, B\n",
    "        ).solution  # (in_features, grid_size + spline_order, out_features)\n",
    "        result = solution.permute(\n",
    "            2, 0, 1\n",
    "        )  # (out_features, in_features, grid_size + spline_order)\n",
    "\n",
    "        assert result.size() == (\n",
    "            self.out_features,\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return result.contiguous()\n",
    "\n",
    "    @property\n",
    "    def scaled_spline_weight(self):\n",
    "        return self.spline_weight * (\n",
    "            self.spline_scaler.unsqueeze(-1)\n",
    "            if self.enable_standalone_scale_spline\n",
    "            else 1.0\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
    "        spline_output = F.linear(\n",
    "            self.b_splines(x).view(x.size(0), -1),\n",
    "            self.scaled_spline_weight.view(self.out_features, -1),\n",
    "        )\n",
    "        return base_output + spline_output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        batch = x.size(0)\n",
    "\n",
    "        splines = self.b_splines(x)  # (batch, in, coeff)\n",
    "        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n",
    "        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n",
    "        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n",
    "        unreduced_spline_output = torch.bmm(\n",
    "            splines, orig_coeff\n",
    "        )  # (in, batch, out)\n",
    "        unreduced_spline_output = unreduced_spline_output.permute(\n",
    "            1, 0, 2\n",
    "        )  # (batch, in, out)\n",
    "\n",
    "        # sort each channel individually to collect data distribution\n",
    "        x_sorted = torch.sort(x, dim=0)[0]\n",
    "        grid_adaptive = x_sorted[\n",
    "            torch.linspace(\n",
    "                0,\n",
    "                batch - 1,\n",
    "                self.grid_size + 1,\n",
    "                dtype=torch.int64,\n",
    "                device=x.device,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        uniform_step = (\n",
    "            x_sorted[-1] - x_sorted[0] + 2 * margin\n",
    "        ) / self.grid_size\n",
    "        grid_uniform = (\n",
    "            torch.arange(\n",
    "                self.grid_size + 1, dtype=torch.float32, device=x.device\n",
    "            ).unsqueeze(1)\n",
    "            * uniform_step\n",
    "            + x_sorted[0]\n",
    "            - margin\n",
    "        )\n",
    "\n",
    "        grid = (\n",
    "            self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
    "        )\n",
    "        grid = torch.concatenate(\n",
    "            [\n",
    "                grid[:1]\n",
    "                - uniform_step\n",
    "                * torch.arange(\n",
    "                    self.spline_order, 0, -1, device=x.device\n",
    "                ).unsqueeze(1),\n",
    "                grid,\n",
    "                grid[-1:]\n",
    "                + uniform_step\n",
    "                * torch.arange(\n",
    "                    1, self.spline_order + 1, device=x.device\n",
    "                ).unsqueeze(1),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        self.grid.copy_(grid.T)  # type: ignore\n",
    "        self.spline_weight.data.copy_(\n",
    "            self.curve2coeff(x, unreduced_spline_output)\n",
    "        )\n",
    "\n",
    "    def regularization_loss(\n",
    "        self, regularize_activation=1.0, regularize_entropy=1.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute the regularization loss.\n",
    "\n",
    "        This is a dumb simulation of the original L1 regularization as\n",
    "        stated in the paper, since the original one requires computing\n",
    "        absolutes and entropy from the expanded\n",
    "        (batch, in_features, out_features) intermediate tensor, which is\n",
    "        hidden behind the F.linear function if we want an memory\n",
    "        efficient implementation.\n",
    "\n",
    "        The L1 regularization is now computed as mean absolute value of the\n",
    "        spline weights. The authors implementation also includes this term\n",
    "        in addition to the sample-based regularization.\n",
    "        \"\"\"\n",
    "        l1_fake = self.spline_weight.abs().mean(-1)\n",
    "        regularization_loss_activation = l1_fake.sum()\n",
    "        p = l1_fake / regularization_loss_activation\n",
    "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
    "        return (\n",
    "            regularize_activation * regularization_loss_activation\n",
    "            + regularize_entropy * regularization_loss_entropy\n",
    "        )\n",
    "\n",
    "\n",
    "class KAN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        width,\n",
    "        grid=3,\n",
    "        k=3,\n",
    "        noise_scale=0.1,\n",
    "        noise_scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        base_fun=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "        bias_trainable=True,\n",
    "    ):\n",
    "        super(KAN, self).__init__()\n",
    "        self.grid_size = grid\n",
    "        self.spline_order = k\n",
    "        self.bias_trainable = bias_trainable  # TODO\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for in_features, out_features in zip(width, width[1:]):\n",
    "            self.layers.append(\n",
    "                KANLinear(\n",
    "                    in_features,\n",
    "                    out_features,\n",
    "                    grid_size=grid,\n",
    "                    spline_order=grid,\n",
    "                    scale_noise=noise_scale,\n",
    "                    scale_base=noise_scale_base,\n",
    "                    scale_spline=scale_spline,\n",
    "                    base_activation=base_fun,\n",
    "                    grid_eps=grid_eps,\n",
    "                    grid_range=grid_range,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, update_grid=False):\n",
    "        B, C, T = x.shape\n",
    "\n",
    "        x = x.view(-1, T)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if update_grid:\n",
    "                layer.update_grid(x)\n",
    "            x = layer(x)\n",
    "\n",
    "        U = x.shape[1]\n",
    "\n",
    "        x = x.view(B, C, U)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def regularization_loss(\n",
    "        self, regularize_activation=1.0, regularize_entropy=1.0\n",
    "    ):\n",
    "        return sum(\n",
    "            layer.regularization_loss(\n",
    "                regularize_activation, regularize_entropy\n",
    "            )\n",
    "            for layer in self.layers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2f8155f-343c-43f2-b451-4572ac1f8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import *\n",
    "import torch\n",
    "from torch import (\n",
    "    nn, \n",
    "    Tensor, \n",
    "    FloatTensor, \n",
    "    LongTensor\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ElectraGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        vocab_type_size: int,\n",
    "        embedding_dropout_p: float,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        ff_dim: int,\n",
    "        num_layers: int,\n",
    "        max_pos_embedding: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = InputEmbedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            vocab_type_size,\n",
    "            embedding_dropout_p,\n",
    "            max_pos_embedding\n",
    "        )\n",
    "        self.encoder = ElectraEncoder(\n",
    "            hidden_dim,\n",
    "            num_heads,\n",
    "            num_layers,\n",
    "            0.1,\n",
    "            ff_dim\n",
    "        )\n",
    "        self.generator = GeneratorOutput(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: LongTensor, \n",
    "        attention_mask: LongTensor,\n",
    "        token_type_ids: LongTensor,\n",
    "    ) -> Tensor:\n",
    "        embeddings = self.embedding(input_ids, token_type_ids)\n",
    "        seq_out = self.encoder(embeddings, attention_mask)\n",
    "        dropouted_seq_output = F.dropout(seq_out, p=0.1)\n",
    "        return self.generator(dropouted_seq_output)\n",
    "    \n",
    "\n",
    "class GeneratorOutput(nn.Module):\n",
    "    def __init__(self, hidden, vocab_size) :\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        return self.softmax(self.linear(x))\n",
    "    \n",
    "    \n",
    "class ElectraDiscriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        vocab_type_size: int,\n",
    "        embedding_dropout_p: float,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        ff_dim: int,\n",
    "        num_layers: int,\n",
    "        max_pos_embedding: int,\n",
    "        num_labels: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = InputEmbedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            vocab_type_size,\n",
    "            embedding_dropout_p,\n",
    "            max_pos_embedding\n",
    "        )\n",
    "        self.encoder = ElectraEncoder(\n",
    "            hidden_dim,\n",
    "            num_heads,\n",
    "            num_layers,\n",
    "            0.1,\n",
    "            ff_dim\n",
    "        )\n",
    "        self.classifier = KAN(width=[hidden_dim, num_labels])\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: LongTensor, \n",
    "        attention_mask: LongTensor,\n",
    "        token_type_ids: LongTensor,\n",
    "    ) -> Tensor:\n",
    "        embeddings = self.embedding(input_ids, token_type_ids)\n",
    "        seq_out = self.encoder(embeddings, attention_mask)\n",
    "        dropouted_seq_output = F.dropout(seq_out, p=0.1)\n",
    "        return self.classifier(dropouted_seq_output)\n",
    "    \n",
    "\n",
    "class ElectraEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        dropout_p: float = 0.1,\n",
    "        hidden_dim: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not hidden_dim:\n",
    "            hidden_dim = dim * 4 # default hidden_dim on paper\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(dim, num_heads, hidden_dim, dropout_p) for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Tensor,\n",
    "        mask: Tensor\n",
    "    ) -> Tensor:\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, mask)\n",
    "        return hidden_states\n",
    "\n",
    "    \n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        vocab_type_size: int,\n",
    "        embedding_dropout_p: float,\n",
    "        max_pos_embedding: int\n",
    "        ):\n",
    "       super().__init__()\n",
    "       self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "       self.positional_embedding = nn.Embedding(max_pos_embedding, embedding_dim)\n",
    "       self.token_type_embedding = nn.Embedding(vocab_type_size, embedding_dim)\n",
    "       self.dropout = nn.Dropout(embedding_dropout_p)\n",
    "   \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: LongTensor, \n",
    "        token_type_ids: LongTensor,\n",
    "    ) -> Tensor:\n",
    "        seq_length = input_ids.shape[1]\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        \n",
    "        embeddings = (\n",
    "            self.embedding(input_ids) +\n",
    "            self.positional_embedding(position_ids) +\n",
    "            self.token_type_embedding(token_type_ids)\n",
    "        )\n",
    "        return self.dropout(embeddings)\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        query: Tensor, \n",
    "        key: Tensor,\n",
    "        value: Tensor,\n",
    "        attention_mask: LongTensor\n",
    "    ) -> Tensor:\n",
    "        multiplied_kv = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(key.shape[-1])\n",
    "        masked_attention = multiplied_kv.masked_fill(attention_mask == 0, -1e9)\n",
    "        attention = self.softmax(masked_attention)\n",
    "        return torch.matmul(attention, value)\n",
    "        \n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        dropout_p: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.attention = ScaledDotProductAttention(dropout_p)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc_q = KAN(width=[dim, dim])\n",
    "        self.fc_k = KAN(width=[dim, dim])\n",
    "        self.fc_v = KAN(width=[dim, dim])\n",
    "        self.fc_out = KAN(width=[dim, dim])\n",
    "        self.num_heads = num_heads \n",
    "        self.dim = dim\n",
    "               \n",
    "    def forward(\n",
    "        self, \n",
    "        query: Tensor, \n",
    "        key: Tensor,\n",
    "        value: Tensor,\n",
    "        attention_mask: LongTensor\n",
    "    ) -> Tensor:\n",
    "        batch_size = query.size(0)\n",
    "        query = self.fc_q(query).view(batch_size, -1, self.num_heads, query.size(-1) // self.num_heads).transpose(1, 2)\n",
    "        key = self.fc_k(key).view(batch_size, -1, self.num_heads, key.size(-1) // self.num_heads).transpose(1, 2)\n",
    "        value = self.fc_v(value).view(batch_size, -1, self.num_heads, value.size(-1) // self.num_heads).transpose(1, 2)\n",
    "        attention_output = self.attention(query, key, value, attention_mask)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * (self.dim // self.num_heads))\n",
    "        output = self.fc_out(attention_output)\n",
    "        return self.dropout(output)\n",
    " \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        ff_dim: int,\n",
    "        dropout_p: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, ff_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ff_dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor\n",
    "    ) -> Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        dropout_p: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(dim, num_heads, dropout_p)\n",
    "        self.ff = FeedForward(dim, hidden_dim, dropout_p)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor, \n",
    "        attention_mask: LongTensor\n",
    "    ) -> Tensor:\n",
    "        attention_output = self.attn(x, x, x, attention_mask)\n",
    "        add_norm = self.norm1(x + attention_output)\n",
    "        output = self.ff(attention_output)\n",
    "        ff_add_norm = self.norm2(add_norm + output)\n",
    "        return self.dropout(ff_add_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6703ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ElectraGenerator(\n",
    "    vocab_size=len(generator_tokenizer.vocab),\n",
    "    embedding_dim=768,\n",
    "    vocab_type_size=2,\n",
    "    embedding_dropout_p=0.1,\n",
    "    hidden_dim=768,\n",
    "    num_heads=12,\n",
    "    ff_dim=3072,\n",
    "    num_layers=12,\n",
    "    max_pos_embedding=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c80e9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(\n",
    "    random_input_ids,\n",
    "    random_attention_mask,\n",
    "    random_token_type_ids\n",
    ")\n",
    "output = torch.argmax(output, axis=2).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "68af37ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5973,\n",
       " 21371,\n",
       " 17665,\n",
       " 2757,\n",
       " 2209,\n",
       " 19749,\n",
       " 7959,\n",
       " 17917,\n",
       " 12997,\n",
       " 4322,\n",
       " 29952,\n",
       " 21913,\n",
       " 22973,\n",
       " 30240,\n",
       " 2771,\n",
       " 16439,\n",
       " 5478,\n",
       " 21305,\n",
       " 21371,\n",
       " 987,\n",
       " 22077,\n",
       " 27747,\n",
       " 9538,\n",
       " 17917,\n",
       " 26847,\n",
       " 17917,\n",
       " 13317,\n",
       " 9410,\n",
       " 29989,\n",
       " 22268,\n",
       " 15036,\n",
       " 11362,\n",
       " 21371,\n",
       " 2548,\n",
       " 21371,\n",
       " 7068,\n",
       " 10746,\n",
       " 26194,\n",
       " 9489,\n",
       " 17919,\n",
       " 22566,\n",
       " 2757,\n",
       " 25107,\n",
       " 6098,\n",
       " 21371,\n",
       " 2757,\n",
       " 26566,\n",
       " 21371,\n",
       " 2757,\n",
       " 2222,\n",
       " 2757,\n",
       " 12319,\n",
       " 29551,\n",
       " 12118,\n",
       " 13117,\n",
       " 16683,\n",
       " 27747,\n",
       " 22547,\n",
       " 21371,\n",
       " 29643,\n",
       " 5344,\n",
       " 2757,\n",
       " 26401,\n",
       " 15036,\n",
       " 7418,\n",
       " 18631,\n",
       " 8118,\n",
       " 18454,\n",
       " 29643,\n",
       " 17917,\n",
       " 17917,\n",
       " 3785,\n",
       " 20191,\n",
       " 2757,\n",
       " 16490,\n",
       " 22056,\n",
       " 21371,\n",
       " 7788,\n",
       " 26315,\n",
       " 23795,\n",
       " 2499,\n",
       " 13394,\n",
       " 23429,\n",
       " 2929,\n",
       " 21371,\n",
       " 7418,\n",
       " 7760,\n",
       " 23149,\n",
       " 2757,\n",
       " 2757,\n",
       " 21371,\n",
       " 16852,\n",
       " 4497,\n",
       " 2757,\n",
       " 23514,\n",
       " 2757,\n",
       " 29230,\n",
       " 29230,\n",
       " 12118,\n",
       " 8714,\n",
       " 7965,\n",
       " 23821,\n",
       " 14089,\n",
       " 12802,\n",
       " 23643,\n",
       " 5149,\n",
       " 21371,\n",
       " 20403,\n",
       " 24995,\n",
       " 22095,\n",
       " 5310,\n",
       " 2222,\n",
       " 15036,\n",
       " 17256,\n",
       " 11355,\n",
       " 21371,\n",
       " 17917,\n",
       " 29230,\n",
       " 5051,\n",
       " 4451,\n",
       " 29230,\n",
       " 2209,\n",
       " 17917,\n",
       " 17421,\n",
       " 10746,\n",
       " 20132,\n",
       " 2757,\n",
       " 17421,\n",
       " 4036,\n",
       " 4497,\n",
       " 7418,\n",
       " 16644,\n",
       " 24894,\n",
       " 2166,\n",
       " 4004,\n",
       " 12394,\n",
       " 7959,\n",
       " 2757,\n",
       " 21371,\n",
       " 17917,\n",
       " 13302,\n",
       " 12460,\n",
       " 9310,\n",
       " 29331,\n",
       " 2757,\n",
       " 16402,\n",
       " 4903,\n",
       " 2773,\n",
       " 22547,\n",
       " 30272,\n",
       " 10746,\n",
       " 29643,\n",
       " 24239,\n",
       " 7825,\n",
       " 2757,\n",
       " 21371,\n",
       " 22095,\n",
       " 24995,\n",
       " 14596,\n",
       " 21371,\n",
       " 7021,\n",
       " 2757,\n",
       " 2757,\n",
       " 5416,\n",
       " 21371,\n",
       " 14729,\n",
       " 10898,\n",
       " 3210,\n",
       " 5149,\n",
       " 13314,\n",
       " 28000,\n",
       " 3620,\n",
       " 7959,\n",
       " 21371,\n",
       " 30064,\n",
       " 6108,\n",
       " 14743,\n",
       " 11775,\n",
       " 24271,\n",
       " 10612,\n",
       " 2033,\n",
       " 29662,\n",
       " 15036,\n",
       " 2222,\n",
       " 15675,\n",
       " 3748,\n",
       " 18936,\n",
       " 2757,\n",
       " 2166,\n",
       " 4451,\n",
       " 2677,\n",
       " 7704,\n",
       " 22547,\n",
       " 25455,\n",
       " 10288,\n",
       " 2209,\n",
       " 30240,\n",
       " 18552,\n",
       " 17238,\n",
       " 18488,\n",
       " 20963,\n",
       " 24271,\n",
       " 26274,\n",
       " 28000,\n",
       " 28000,\n",
       " 15675,\n",
       " 12412,\n",
       " 2757,\n",
       " 9808,\n",
       " 27945,\n",
       " 2757,\n",
       " 13314,\n",
       " 21371,\n",
       " 4875,\n",
       " 3960,\n",
       " 23345,\n",
       " 29643,\n",
       " 8456,\n",
       " 2757,\n",
       " 22192,\n",
       " 7959,\n",
       " 9434,\n",
       " 28994,\n",
       " 17421,\n",
       " 3210,\n",
       " 12058,\n",
       " 25292,\n",
       " 10906,\n",
       " 21371,\n",
       " 26368,\n",
       " 12514,\n",
       " 21371,\n",
       " 15036,\n",
       " 27002,\n",
       " 25329,\n",
       " 2499,\n",
       " 21371,\n",
       " 15036,\n",
       " 26240,\n",
       " 17919,\n",
       " 22774,\n",
       " 22774,\n",
       " 22890,\n",
       " 21371,\n",
       " 9366,\n",
       " 14312,\n",
       " 748,\n",
       " 21371,\n",
       " 13961,\n",
       " 26221,\n",
       " 12645,\n",
       " 7124,\n",
       " 9056,\n",
       " 13170,\n",
       " 1803,\n",
       " 29547,\n",
       " 21371,\n",
       " 21371,\n",
       " 21371,\n",
       " 27672,\n",
       " 29331,\n",
       " 6462,\n",
       " 15036,\n",
       " 15969,\n",
       " 2757,\n",
       " 1546,\n",
       " 21820,\n",
       " 2757,\n",
       " 21371,\n",
       " 30359,\n",
       " 4584,\n",
       " 20053,\n",
       " 26060,\n",
       " 21371,\n",
       " 23825,\n",
       " 4062,\n",
       " 19934,\n",
       " 2105,\n",
       " 2757,\n",
       " 21371,\n",
       " 2291,\n",
       " 10719,\n",
       " 7418,\n",
       " 15036,\n",
       " 7600,\n",
       " 17642,\n",
       " 2757,\n",
       " 14580,\n",
       " 21371,\n",
       " 2757,\n",
       " 2757,\n",
       " 15036,\n",
       " 2222,\n",
       " 21371,\n",
       " 19818,\n",
       " 12429,\n",
       " 21371,\n",
       " 28546,\n",
       " 2757,\n",
       " 14211,\n",
       " 17421,\n",
       " 22520,\n",
       " 2222,\n",
       " 18938,\n",
       " 22774,\n",
       " 13761,\n",
       " 29643,\n",
       " 2757,\n",
       " 8456,\n",
       " 18051,\n",
       " 12708,\n",
       " 18541,\n",
       " 2757,\n",
       " 2757,\n",
       " 17917,\n",
       " 20515,\n",
       " 17917,\n",
       " 3210,\n",
       " 20403,\n",
       " 21371,\n",
       " 26692,\n",
       " 15004,\n",
       " 4497,\n",
       " 30240,\n",
       " 21371,\n",
       " 4078,\n",
       " 7068,\n",
       " 23345,\n",
       " 2757,\n",
       " 2757,\n",
       " 973,\n",
       " 24880,\n",
       " 2209,\n",
       " 17917,\n",
       " 25852,\n",
       " 2757,\n",
       " 9634,\n",
       " 14526,\n",
       " 2757,\n",
       " 20191,\n",
       " 3210,\n",
       " 17256,\n",
       " 20191,\n",
       " 14312,\n",
       " 2757,\n",
       " 7915,\n",
       " 29331,\n",
       " 28128,\n",
       " 2499,\n",
       " 4497,\n",
       " 647,\n",
       " 5479,\n",
       " 7760,\n",
       " 2757,\n",
       " 13490,\n",
       " 21971,\n",
       " 22831,\n",
       " 24208,\n",
       " 100,\n",
       " 13604,\n",
       " 22056,\n",
       " 29662,\n",
       " 29662,\n",
       " 7916,\n",
       " 2291,\n",
       " 21371,\n",
       " 23412,\n",
       " 9328,\n",
       " 22056,\n",
       " 17917,\n",
       " 2757,\n",
       " 17421,\n",
       " 22056,\n",
       " 6208,\n",
       " 27747,\n",
       " 22774,\n",
       " 23821,\n",
       " 21371,\n",
       " 23097,\n",
       " 29246,\n",
       " 8138,\n",
       " 663,\n",
       " 17264,\n",
       " 8126,\n",
       " 12504,\n",
       " 17917,\n",
       " 2757,\n",
       " 353,\n",
       " 22436,\n",
       " 3620,\n",
       " 21371,\n",
       " 6862,\n",
       " 147,\n",
       " 9460,\n",
       " 12429,\n",
       " 2757,\n",
       " 7760,\n",
       " 2757,\n",
       " 2757,\n",
       " 11205,\n",
       " 20191,\n",
       " 14211,\n",
       " 13958,\n",
       " 29986,\n",
       " 22174,\n",
       " 17256,\n",
       " 24447,\n",
       " 16110,\n",
       " 28441,\n",
       " 8126,\n",
       " 18217,\n",
       " 14784,\n",
       " 21371,\n",
       " 14730,\n",
       " 7368,\n",
       " 17917,\n",
       " 30255,\n",
       " 29938,\n",
       " 15036,\n",
       " 28504,\n",
       " 25612,\n",
       " 5072,\n",
       " 19975,\n",
       " 21371,\n",
       " 21371,\n",
       " 23595,\n",
       " 2757,\n",
       " 26457,\n",
       " 3620,\n",
       " 2757,\n",
       " 22056,\n",
       " 21371,\n",
       " 29643,\n",
       " 6057,\n",
       " 15675,\n",
       " 15036,\n",
       " 21371,\n",
       " 16290,\n",
       " 21371,\n",
       " 9310,\n",
       " 13298,\n",
       " 337,\n",
       " 12714,\n",
       " 22324,\n",
       " 23086,\n",
       " 17737,\n",
       " 2757,\n",
       " 1431,\n",
       " 5344,\n",
       " 27353,\n",
       " 30255,\n",
       " 3785,\n",
       " 23892,\n",
       " 2757,\n",
       " 24894,\n",
       " 13282,\n",
       " 21305,\n",
       " 7959,\n",
       " 21371,\n",
       " 16674,\n",
       " 17341,\n",
       " 29867,\n",
       " 6079,\n",
       " 2757,\n",
       " 16993,\n",
       " 2757,\n",
       " 7068,\n",
       " 2222,\n",
       " 3785,\n",
       " 3874,\n",
       " 22056,\n",
       " 9177,\n",
       " 7704,\n",
       " 14032,\n",
       " 2757,\n",
       " 12588,\n",
       " 4451,\n",
       " 2757,\n",
       " 21371,\n",
       " 5344,\n",
       " 2757,\n",
       " 7418,\n",
       " 21371,\n",
       " 27876,\n",
       " 22501,\n",
       " 17049,\n",
       " 17242,\n",
       " 2757,\n",
       " 425,\n",
       " 29372,\n",
       " 2757,\n",
       " 2757,\n",
       " 29952,\n",
       " 17421,\n",
       " 23864,\n",
       " 18912,\n",
       " 2757,\n",
       " 24094,\n",
       " 2757,\n",
       " 17917,\n",
       " 6873,\n",
       " 8388,\n",
       " 2757,\n",
       " 3620,\n",
       " 15703,\n",
       " 2757,\n",
       " 17603,\n",
       " 6057,\n",
       " 22547,\n",
       " 29986,\n",
       " 24219,\n",
       " 7600]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1b358107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "def initialize_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Conv2d):\n",
    "        init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
    "        if module.bias is not None:\n",
    "            init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        init.normal_(module.weight, mean=0, std=0.01)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        init.ones_(module.weight)\n",
    "        init.zeros_(module.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f5743f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained('google/electra-base-generator')\n",
    "model.apply(initialize_weights)\n",
    "output_orig = model(input_ids=random_input_ids, attention_mask=random_attention_mask, token_type_ids=random_token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01ea5338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 30522])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_orig.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "96682b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4367,  4367,  4367,  4367,  4367,  4367,  4367,  4367,  4367,  4367,\n",
       "          4367,  4367,  4367,  4367,  4367, 14611, 14611, 14611,  4367, 14611,\n",
       "         14611,  4367,  4367, 14611,  4367, 14611,  4367,  4367,  4367,  4367,\n",
       "          4367, 14611,  4367, 14611,  4367,  4367,  4367,  4367, 14611,  4367,\n",
       "          4367, 14611,  4367,  4367,  4367,  4367, 14611,  4367,  4367,  4367,\n",
       "          4367,  4367,  4367, 14611, 14611, 14611,  4367,  4367, 14611,  4367,\n",
       "          4367, 14611,  4367, 14611,  4367, 14611,  4367,  4367,  4367, 14611,\n",
       "          4367,  4367,  4367,  4367,  4367, 14611, 14611, 14611,  4367,  4367,\n",
       "         14611,  4367,  4367,  4367,  4367, 14611, 14611,  4367, 14611, 14611,\n",
       "          4367,  4367, 14611, 14611,  4367,  4367,  4367,  4367,  4367, 14611,\n",
       "         14611, 14611,  4367, 14611,  4367,  4367, 14611, 14611,  4367,  4367,\n",
       "         14611, 14611,  4367, 14611,  4367,  4367,  4367,  4367, 14611,  4367,\n",
       "         14611,  4367,  4367,  4367,  4367, 14611, 14611, 14611, 14611,  4367,\n",
       "         14611,  4367,  4367,  4367,  4367, 14611, 14611,  4367, 14611,  4367,\n",
       "          4367,  4367,  4367,  4367,  4367,  4367, 14611, 14611,  4367,  4367,\n",
       "         14611, 14611, 14611, 14611,  4367,  4367,  4367,  4367,  4367, 14611,\n",
       "         14611, 14611,  4367, 14611,  4367, 14611, 14611,  4367,  4367,  4367,\n",
       "          4367,  4367,  4367,  4367,  4367,  4367,  4367, 14611, 14611, 14611,\n",
       "          4367,  4367,  4367,  4367,  4367, 14611,  4367,  4367, 14611, 14611,\n",
       "         14611, 14611,  4367,  4367, 14611, 14611,  4367,  4367, 14611,  4367,\n",
       "          4367, 14611, 14611, 14611, 14611,  4367, 14611,  4367,  4367,  4367,\n",
       "         14611, 14611, 14611,  4367,  4367, 14611, 14611, 14611,  4367, 14611,\n",
       "          4367, 14611, 14611,  4367,  4367,  4367,  4367,  4367, 14611, 14611,\n",
       "         14611,  4367, 14611,  4367,  4367, 14611, 14611,  4367, 14611, 14611,\n",
       "         14611,  4367, 14611,  4367,  4367, 14611,  4367, 14611, 14611,  4367,\n",
       "          4367, 14611,  4367, 14611, 14611,  4367, 14611, 14611,  4367,  4367,\n",
       "         14611, 14611, 14611, 14611,  4367, 14611, 14611,  4367, 14611, 14611,\n",
       "          4367, 14611, 14611,  4367,  4367,  4367,  4367, 14611,  4367, 14611,\n",
       "         14611, 14611,  4367, 14611,  4367,  4367, 14611, 14611,  4367, 14611,\n",
       "         14611,  4367,  4367,  4367,  4367,  4367,  4367,  4367,  4367, 14611,\n",
       "         14611, 14611, 14611, 14611,  4367,  4367,  4367,  4367,  4367,  4367,\n",
       "          4367,  4367,  4367, 14611, 14611, 14611,  4367,  4367,  4367,  4367,\n",
       "          4367, 14611, 14611, 14611,  4367, 14611, 14611,  4367, 14611,  4367,\n",
       "          4367, 14611, 14611,  4367,  4367, 14611,  4367,  4367, 14611, 14611,\n",
       "          4367, 14611,  4367, 14611, 14611,  4367, 14611,  4367,  4367, 14611,\n",
       "         14611,  4367, 14611,  4367,  4367, 14611,  4367, 14611,  4367,  4367,\n",
       "         14611,  4367,  4367,  4367,  4367,  4367, 14611,  4367,  4367, 14611,\n",
       "          4367,  4367,  4367, 14611, 14611, 14611,  4367, 14611, 14611, 14611,\n",
       "          4367,  4367, 14611,  4367, 14611,  4367,  4367,  4367,  4367,  4367,\n",
       "         14611,  4367, 14611, 14611, 14611, 14611,  4367,  4367, 14611,  4367,\n",
       "          4367,  4367,  4367,  4367, 14611,  4367, 14611, 14611,  4367, 14611,\n",
       "         14611,  4367, 14611, 14611,  4367, 14611,  4367,  4367,  4367, 14611,\n",
       "          4367,  4367, 14611, 14611, 14611,  4367, 14611, 14611, 14611, 14611,\n",
       "          4367, 14611, 14611,  4367,  4367, 14611, 14611,  4367, 14611, 14611,\n",
       "         14611, 14611,  4367, 14611, 14611,  4367, 14611,  4367,  4367,  4367,\n",
       "         14611,  4367, 14611, 14611,  4367, 14611,  4367,  4367,  4367, 14611,\n",
       "          4367,  4367,  4367, 14611,  4367, 14611, 14611,  4367, 14611,  4367,\n",
       "         14611,  4367,  4367, 14611, 14611, 14611,  4367, 14611,  4367, 14611,\n",
       "          4367,  4367,  4367,  4367,  4367, 14611, 14611, 14611, 14611, 14611,\n",
       "         14611,  4367,  4367, 14611,  4367, 14611, 14611, 14611, 14611, 14611,\n",
       "         14611, 14611, 14611,  4367, 14611,  4367,  4367, 14611,  4367,  4367,\n",
       "          4367,  4367]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(output_orig.logits, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf21a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(random_input_ids, random_attention_mask, random_token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c46161ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'infrared',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_tokenizer.convert_ids_to_tokens(torch.argmax(output, axis=2).squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "208bb447-505d-469f-b00a-072d569da746",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generator_tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[0;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "generator_tokenizer.convert_ids_to_tokens(torch.argmax(output, axis=2).squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd4989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
